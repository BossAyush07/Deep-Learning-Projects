{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Study of NLTK library.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMkqKO9stw+5HL1bEkv3ED",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BossAyush07/Deep-Learning-Projects/blob/master/Study_of_NLTK_library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4KaTiIFb1FR",
        "colab_type": "text"
      },
      "source": [
        "NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLPNaf3mcBEJ",
        "colab_type": "text"
      },
      "source": [
        "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language.Tokenization, Stemming, Lemmatization, Punctuation, Character count, word count are some of these packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3Zc5ls9bZXR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "c89e4d52-b55c-434e-f564-70768c10f805"
      },
      "source": [
        "import nltk\n",
        "# the below three lines of code is google colab specific we don't require it for normal anaconda jupyter notebook\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL97ABckcco2",
        "colab_type": "text"
      },
      "source": [
        "WORD TOKENIZE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sswn0B7NcnfC",
        "colab_type": "text"
      },
      "source": [
        "The method word_tokenize() is used to split a sentence into words. The output of word tokenization can be converted to Data Frame for better text understanding in machine learning applications. It can also be provided as input for further text cleaning steps such as punctuation removal, numeric character removal or stemming. Machine learning models need numeric data to be trained and make a prediction. Word tokenization becomes a crucial part of the text (string) to numeric data conversion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi3IR0TgcUOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfef4593-b109-41ad-edae-47846d2cc956"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Sun rises from east\"\n",
        "print(word_tokenize(text))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sun', 'rises', 'from', 'east']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSc_3PYGdndR",
        "colab_type": "text"
      },
      "source": [
        "SENTENCE TOKENIZE:\n",
        "\n",
        "The method sent_tokenize() is used to slpit sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAtt0OyudMsU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff7d6a37-6ff9-44bb-967b-57e6d0580a07"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Sunrises from East !  Dehradun is capital of Uttarakhand\"\n",
        "print(sent_tokenize(text))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sunrises from East !', 'Dehradun is capital of Uttarakhand']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUsceJ6id-JS",
        "colab_type": "text"
      },
      "source": [
        "POS Tagging\n",
        "\n",
        "Parts of speech Tagging is responsible for reading the text in a language and assigning some specific token (Parts of Speech) to each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECl5JGR9d41M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "1e6352b9-798f-4a4e-961d-8d737bb7de10"
      },
      "source": [
        "token = word_tokenize(text)\n",
        "nltk.pos_tag(token)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Sunrises', 'NNS'),\n",
              " ('from', 'IN'),\n",
              " ('East', 'NNP'),\n",
              " ('!', '.'),\n",
              " ('Dehradun', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('capital', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('Uttarakhand', 'NNP')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnDORt43e5lx",
        "colab_type": "text"
      },
      "source": [
        "NLTK  STEMMING\n",
        "\n",
        "Stemming means normalization of words. Normalization is a method where a list of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.\n",
        "A word stem is part of a word. It is sort of a normalization idea, but linguistic.\n",
        "For example, the stem of the word waiting is wait.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsFq4CyCemvV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "b8dbb537-8811-42bb-922c-64245f126c1e"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        " \n",
        "words = [\"game\",\"gaming\",\"gamed\",\"games\"]\n",
        "ps = PorterStemmer()\n",
        "for i in words:\n",
        "  print(ps.stem(i))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "game\n",
            "game\n",
            "game\n",
            "game\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOe4ySRIgENP",
        "colab_type": "text"
      },
      "source": [
        "LEMMATIZATION\n",
        "\n",
        "Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as the lemma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1vjPHaOfHlh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "bcffd53e-79de-4879-a60b-6602964ce994"
      },
      "source": [
        "from nltk.stem import \tWordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "\tprint(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  \n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemma for studies is study\n",
            "Lemma for studying is studying\n",
            "Lemma for cries is cry\n",
            "Lemma for cry is cry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR-0x_Dkgp7o",
        "colab_type": "text"
      },
      "source": [
        "CHUNCKING\n",
        "\n",
        "Chunking is used to add more structure to the sentence by following parts of speech (POS) tagging. It is also known as shallow parsing. The resulted group of words is called chunks. In shallow parsing, there is maximum one level between roots and leaves while deep parsing comprises of more than one level. Shallow Parsing is also called light parsing or chunking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpGqtpClgXRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}